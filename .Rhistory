library(mlbench)
library(e1071)
library(caret)
data(BreastCancer)
raw.data <- BreastCancer
## Count number of missing values
sapply(raw.data, function(x) sum(is.na(x)))
## We are missing 16 elements in the column Bare.nuclei.
table(raw.data$Bare.nuclei)
# This is a  column of type factor, with levels from 1 to 10 will add the new type 11
levels(raw.data$Bare.nuclei) <- c(levels(raw.data$Bare.nuclei), 11)
raw.data$Bare.nuclei[is.na(raw.data$Bare.nuclei)] <- as.factor(11)
table(raw.data$Bare.nuclei)
# Remove column ID
raw.data <- raw.data[,2:11]
# Function to calculate SVM
calculateSVM <- function(raw.data, t){
# Divide test set and training set according to proportion t
sub = sample(nrow(raw.data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(raw.data[sub,])
testData <- data.frame(raw.data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
return(result$overall['Accuracy'])
}
calculateSVM(raw.data, t)
# Divide test set and training set according to proportion t
sub <- sample(nrow(raw.data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(raw.data[sub,])
testData <- data.frame(raw.data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
return(result$overall['Accuracy'])
}
calculateSVM <- function(data, t){
# Divide test set and training set according to proportion t
sub <- sample(nrow(data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(data[sub,])
testData <- data.frame(data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
return(result$overall['Accuracy'])
}
library(mlbench)
library(e1071)
library(caret)
data(BreastCancer)
raw.data <- BreastCancer
## Count number of missing values
sapply(raw.data, function(x) sum(is.na(x)))
## We are missing 16 elements in the column Bare.nuclei.
table(raw.data$Bare.nuclei)
# This is a  column of type factor, with levels from 1 to 10 will add the new type 11
levels(raw.data$Bare.nuclei) <- c(levels(raw.data$Bare.nuclei), 11)
raw.data$Bare.nuclei[is.na(raw.data$Bare.nuclei)] <- as.factor(11)
table(raw.data$Bare.nuclei)
# Remove column ID
raw.data <- raw.data[,2:11]
# Function to calculate SVM
calculateSVM <- function(data, t){
# Divide test set and training set according to proportion t
sub <- sample(nrow(data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(data[sub,])
testData <- data.frame(data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
return(result$overall['Accuracy'])
}
calculateSVM(raw.data, t)
t <- as.numeric(0.7)
calculateSVM(raw.data, t)
N <- calculateSVM(raw.data, t)
n
N
result
N <- calculateSVM(raw.data, t)
library(mlbench)
library(e1071)
library(caret)
data(BreastCancer)
raw.data <- BreastCancer
## Count number of missing values
sapply(raw.data, function(x) sum(is.na(x)))
## We are missing 16 elements in the column Bare.nuclei.
table(raw.data$Bare.nuclei)
# This is a  column of type factor, with levels from 1 to 10 will add the new type 11
levels(raw.data$Bare.nuclei) <- c(levels(raw.data$Bare.nuclei), 11)
raw.data$Bare.nuclei[is.na(raw.data$Bare.nuclei)] <- as.factor(11)
table(raw.data$Bare.nuclei)
# Remove column ID
raw.data <- raw.data[,2:11]
# Function to calculate SVM
calculateSVM <- function(data, t){
# Divide test set and training set according to proportion t
sub <- sample(nrow(data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(data[sub,])
testData <- data.frame(data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
result
return(result$overall['Accuracy'])
}
t <- as.numeric(0.7)
N <- calculateSVM(raw.data, t)
N
library(mlbench)
library(e1071)
library(caret)
data(BreastCancer)
raw.data <- BreastCancer
## Count number of missing values
sapply(raw.data, function(x) sum(is.na(x)))
## We are missing 16 elements in the column Bare.nuclei.
table(raw.data$Bare.nuclei)
# This is a  column of type factor, with levels from 1 to 10 will add the new type 11
levels(raw.data$Bare.nuclei) <- c(levels(raw.data$Bare.nuclei), 11)
raw.data$Bare.nuclei[is.na(raw.data$Bare.nuclei)] <- as.factor(11)
table(raw.data$Bare.nuclei)
# Remove column ID
raw.data <- raw.data[,2:11]
# Function to calculate SVM
calculateSVM <- function(data, t){
# Divide test set and training set according to proportion t
sub <- sample(nrow(data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(data[sub,])
testData <- data.frame(data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
result
return(result)
}
t <- as.numeric(0.7)
N <- calculateSVM(raw.data, t)
N
N
N$positive
N$overall
N <- as.numeric(5)
accB <- list()
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = c(accB, res$overall[1])
}
accB
N <- as.numeric(5)
accB <- data.frame()
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = c(accB, res$overall[1])
}
accB
N <- as.numeric(5)
accB <- data.frame()
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = rbind(accB, res$overall[1])
}
accB
colnames(accB, "accuracy")
colnames(accB, c("accuracy"))
accB <- data.frame("Accuracy")
accB
?data.frame
accB <- data.frame(row.names = "Accuracy")
accB
accB <- data.frame(col.names = "Accuracy")
accB
accB <- data.frame(col.names = NA)
accB
accB <- data.frame(Accuracy)
accB <- data.frame(Accuracy = NA)
accB
accB <- data.frame()
accB <- data.frame()
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = rbind(accB, res$overall[1])
}
accB
plot(accB)
histogram(accB)
mean(accB)
accB[1,1]
accB <- data.frame()
colnames(accB, 'accuracy')
accB <- data.frame()
colnames(accB, 'accuracy')
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = rbind(accB, res$overall[1])
}
accB
sapply(accB, class)
means(accB)
mean(accB)
summary(accB)
sapply(accB, mean)
sapply(accB, sd)
colnames(accB, 'a')
?colnames
colnames(accB) <- 'accuracy'
sapply(accB, mean)
sapply(accB, sd)
library(ggplot)
library(ggplot2)
qplot(accB, geom='histogram')
qplot(accB$accuracy, geom='histogram')
# Part b
## Number of repetitions and reset data frame for storing values
N <- as.numeric(50)
accB <- data.frame()
## Loop
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = rbind(accB, res$overall[1])
}
## Summarize
colnames(accB) <- 'accuracy'
qplot(accB$accuracy, geom='histogram')
sapply(accB, mean)
sapply(accB, sd)
library(mlbench)
library(e1071)
library(caret)
library(ggplot2)
data(BreastCancer)
raw.data <- BreastCancer
## Count number of missing values
sapply(raw.data, function(x) sum(is.na(x)))
## We are missing 16 elements in the column Bare.nuclei.
table(raw.data$Bare.nuclei)
# This is a  column of type factor, with levels from 1 to 10 will add the new type 11
levels(raw.data$Bare.nuclei) <- c(levels(raw.data$Bare.nuclei), 11)
raw.data$Bare.nuclei[is.na(raw.data$Bare.nuclei)] <- as.factor(11)
table(raw.data$Bare.nuclei)
# Remove column ID
raw.data <- raw.data[,2:11]
# Function to calculate SVM
calculateSVM <- function(data, t){
# Divide test set and training set according to proportion t
sub <- sample(nrow(data), size = floor(nrow(raw.data) * t))
trainData <- data.frame(data[sub,])
testData <- data.frame(data[-sub,])
# Fitting best SVM through cross validation
tune.out=tune(svm, Class~., data = trainData ,
kernel="linear", ranges=list(
cost=c(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100)))
bestmod=tune.out$best.model
# Predicting values in the test set,
ypred=predict(bestmod,testData)
result = confusionMatrix(ypred, testData$Class)
return(result)
}
# Part a
t <- as.numeric(0.7)
acc <- calculateSVM(raw.data, t)
# Part b
## Number of repetitions and reset data frame for storing values
N <- as.numeric(50)
accB <- data.frame()
## Loop
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accB = rbind(accB, res$overall[1])
}
## Summarize
colnames(accB) <- 'accuracy'
qplot(accB$accuracy, geom='histogram')
sapply(accB, mean)
sapply(accB, sd)
?sapply
?qplot
qplot(accB$accuracy, geom='histogram', main='Test Set Errrors for N=50 experiments of SVM')
?seq
t.values = seq(from = 0.55 to = 0.95 by = 0.05)
t.values = seq(from = 0.55, to = 0.95, by = 0.05)
t.values <- seq(from = 0.50, to = 0.95, by = 0.05)
t.values <- seq(from = 0.70, to = 0.95, by = 0.05)
t.values <- seq(from = 0.80, to = 0.95, by = 0.05)
accC <- data.frame()
means.sd <- data.frame()
# Part c
## Create sequence of t's
t.values <- seq(from = 0.80, to = 0.95, by = 0.05)
## Initialize data frames
accC <- data.frame()
means.sd <- data.frame()
## Loop and record data
for(t in t.values){
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC <- rbind(accC, res$overall[1])
}
means.sd <- rbind(means.sd, c(t, sapply(accC, mean), sapply(accC, sd)))
}
length(t.values)
means.sd <- matrix(ncol = 3, nrow = length(t.values))
accC <- matrix(ncol = 1, nrow = N)
# Part c
## Create sequence of t's
N <- as.numeric(10)
t.values <- seq(from = 0.80, to = 0.95, by = 0.05)
## Initialize data matrix
means.sd <- matrix(ncol = 3, nrow = length(t.values))
j <- as.numeric(1)
## Loop and record data
for(t in t.values){
# Reset matrix
accC <- matrix(ncol = 1, nrow = N)
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC[i] <- res$overall[1]
}
means.sd[j,] <- c(t, sapply(accC, mean), sapply(accC, sd))
j <- j+1
}
means.sd
N <- as.numeric(10)
t.values <- seq(from = 0.80, to = 0.95, by = 0.05)
## Initialize data matrix
means.sd <- matrix(ncol = 3, nrow = length(t.values))
j <- as.numeric(1)
## Loop and record data
for(t in t.values){
# Reset matrix
accC <- matrix(ncol = 1, nrow = N)
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC[i] <- res$overall[1]
}
means.sd[j,1] <- t
means.sd[j,2] <- sapply(accC, mean)
means.sd[j,3] <- sapply(accC, sd)
j <- j+1
}
means.sd
sapply(accC, sd)
# Part c
## Create sequence of t's
N <- as.numeric(10)
t.values <- seq(from = 0.80, to = 0.95, by = 0.05)
## Initialize data matrix
means.sd <- matrix(ncol = 3, nrow = length(t.values))
j <- as.numeric(1)
## Loop and record data
for(t in t.values){
# Reset matrix
accC <- matrix(ncol = 1, nrow = N)
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC[i,1] <- res$overall[1]
}
means.sd[j,1] <- t
means.sd[j,2] <- sapply(accC, mean)
means.sd[j,3] <- sapply(accC, sd)
j <- j+1
}
means.sd
sapply(accC, mean)
mean(accC
mean(accC)
mean(accC)
# Part c
## Create sequence of t's
N <- as.numeric(10)
t.values <- seq(from = 0.80, to = 0.95, by = 0.05)
## Initialize data matrix
means.sd <- matrix(ncol = 3, nrow = length(t.values))
j <- as.numeric(1)
## Loop and record data
for(t in t.values){
# Reset matrix
accC <- matrix(ncol = 1, nrow = N)
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC[i,1] <- res$overall[1]
}
means.sd[j,1] <- t
means.sd[j,2] <- mean(accC)
means.sd[j,3] <- sd(accC)
j <- j+1
}
means.sd
plot(means.sd[1], means.sd[2])
plot(means.sd[,1], means.sd[,2])
?polygon
?rev
plot(means.sd[,1], means.sd[,2])
polygon(c(rev(means.sd[,1]), means.sd[,1]), c(rev(means.sd[,2]), means.sd[,2]), col = 'grey80', border = NA)
polygon(c(rev(means.sd[,1]), means.sd[,1]), c(rev(means.sd[,2]), means.sd[,2]), col = 'grey80', border = NA)
polygon(c(rev(means.sd[,1]), means.sd[,1]), c(rev(means.sd[,1]+means.sd[,2]), means.sd[,1]-means.sd[,2]), col = 'grey80', border = NA)
plot(means.sd[,1], means.sd[,2])
polygon(c(rev(means.sd[,1]), means.sd[,1]), c(rev(means.sd[,1]+means.sd[,2]), means.sd[,1]-means.sd[,2]), col = 'grey80', border = NA)
polygon(c(means.sd[,1], means.sd[,1]), c(rev(means.sd[,1]+means.sd[,2]), means.sd[,1]-means.sd[,2]), col = 'grey80', border = NA)
plot(means.sd[,1], means.sd[,2])
polygon(c(means.sd[,1], means.sd[,1]), c(rev(means.sd[,1]+means.sd[,2]), means.sd[,1]-means.sd[,2]), col = 'grey80', border = NA)
plot(means.sd[,1], means.sd[,2])
polygon(c(means.sd[,1], means.sd[,1]), c(means.sd[,1]+means.sd[,2], means.sd[,1]-means.sd[,2]), col = 'grey80', border = NA)
plot(means.sd[,1], means.sd[,2])
polygon(c(rev(means.sd[,1]), means.sd[,1]), c(rev(means.sd[,1]+means.sd[,2]), means.sd[,1]-means.sd[,2]), col = 'grey80', border = NA)
plot(means.sd[,1], means.sd[,2])
plot(means.sd[,1], means.sd[,2])
lines(means.sd[,1],means.sd[,2]+means.sd[j,3],col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-means.sd[j,3],col="red",lty=2)
plot(means.sd[,1], means.sd[,2])
lines(means.sd[,1],means.sd[,2]+means.sd[,3],col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-means.sd[,3],col="red",lty=2)
?plot
plot(means.sd[,1], means.sd[,2], ylim = c(0.5, 1))
lines(means.sd[,1],means.sd[,2]+means.sd[,3],col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-means.sd[,3],col="red",lty=2)
plot(means.sd[,1], means.sd[,2], ylim = c(0.5, 1))
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
N <- as.numeric(10)
t.values <- seq(from = 0.5, to = 0.95, by = 0.05)
## Initialize data matrix
means.sd <- matrix(ncol = 3, nrow = length(t.values))
j <- as.numeric(1)
## Loop and record data
for(t in t.values){
# Reset matrix
accC <- matrix(ncol = 1, nrow = N)
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC[i,1] <- res$overall[1]
}
means.sd[j,1] <- t
means.sd[j,2] <- mean(accC)
means.sd[j,3] <- sd(accC)
j <- j+1
}
plot(means.sd[,1], means.sd[,2], ylim = c(0.5, 1))
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
# Part c
## Create sequence of t's
N <- as.numeric(50)
t.values <- seq(from = 0.5, to = 0.95, by = 0.05)
## Initialize data matrix
means.sd <- matrix(ncol = 3, nrow = length(t.values))
j <- as.numeric(1)
## Loop and record data
for(t in t.values){
# Reset matrix
accC <- matrix(ncol = 1, nrow = N)
for(i in 1:N){
res <- calculateSVM(raw.data, t)
accC[i,1] <- res$overall[1]
# display progress
cat("loop", t, "rep ", i, "\n")
flush.console()
}
means.sd[j,1] <- t
means.sd[j,2] <- mean(accC)
means.sd[j,3] <- sd(accC)
j <- j+1
}
plot(means.sd[,1], means.sd[,2], ylim = c(0.5, 1))
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
plot(means.sd[,1], means.sd[,2], ylim = c(0.9, 1))
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
?plot
plot(means.sd[,1], means.sd[,2], ylim = c(0.9, 1), xlab = 't', ylab = 'accuracy')
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
plot(means.sd[,1], means.sd[,2], ylim = c(0.9, 1), xlab = 't', ylab = 'accuracy',
main = 'Mean Accuracy for different sizes of training set (t%) ')
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
plot(means.sd[,1], means.sd[,2], ylim = c(0.9, 1), xlab = 't', ylab = 'accuracy',
main = 'Mean Accuracy of SVM for different sizes of training set (t%) ')
lines(means.sd[,1],means.sd[,2]+ 1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
lines(means.sd[,1],means.sd[,2]-1.96*means.sd[,3]/sqrt(N),col="red",lty=2)
